{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 13: Entity & Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Relation extraction from Wikipedia articles\n",
    "\n",
    "Use Wikipedia to extract the relation `directedBy(Movie, Person)` by applying pattern based heuristics that utilize: *Part Of Speech Tagging*, *Named Entity Recognition* and *Regular Expressions*.\n",
    "\n",
    "#### Required Library: SpaCy\n",
    "- ```conda install -y spacy```\n",
    "- ```python -m spacy download en```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json, csv, re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read tsv with input movies\n",
    "def read_tsv():\n",
    "    movies=[]\n",
    "    with open('movies.tsv','r') as file:\n",
    "        tsv = csv.reader(file, delimiter='\\t')\n",
    "        next(tsv) #remove header\n",
    "        movies = [{'movie':line[0], 'director':line[1]} for line in tsv]\n",
    "    return movies\n",
    "\n",
    "#parse wikipedia page\n",
    "def parse_wikipedia(movie):\n",
    "    txt = ''\n",
    "    try:\n",
    "        with urllib.request.urlopen('https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro=&explaintext=&titles='+movie) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            txt = next (iter (data['query']['pages'].values()))['extract']\n",
    "    except:\n",
    "        pass\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = parse_wikipedia('Inception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Parse the raw text of a Wikipedia movie page and extract named (PER) entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_PER_entities(txt):\n",
    "    persons = []\n",
    "    doc = nlp(txt)\n",
    "\n",
    "    for e in doc.ents: \n",
    "        if e.label_ == 'PERSON':\n",
    "            persons.append(e.text)\n",
    "    return set(persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Christopher Nolan',\n",
       " 'Cillian Murphy',\n",
       " 'Dileep Rao',\n",
       " 'Elliot Page',\n",
       " 'Emma Thomas',\n",
       " 'Joseph Gordon-Levitt',\n",
       " 'Ken Watanabe',\n",
       " 'Leonardo DiCaprio',\n",
       " 'Marion Cotillard',\n",
       " 'Michael Caine',\n",
       " 'The Dark Knight',\n",
       " 'Tom Berenger',\n",
       " 'Tom Hardy'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_PER_entities(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Given the raw text of a Wikipedia movie page and the extracted PER entities, find the director."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_director(txt, persons):\n",
    "    # hint from solution: find persons after the word directed\n",
    "    for index, word in enumerate(txt.split()):\n",
    "        if word == 'directed':\n",
    "            remaining = ' '.join(txt.split()[index+1:])\n",
    "            for ii in range(len(remaining.split())):\n",
    "                for person in persons: \n",
    "                    if remaining.startswith(person): \n",
    "                        return person\n",
    "                remaining = ' '.join(remaining.split()[1:])\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'movie': '13_Assassins_(2010_film)', 'director': 'Takashi Miike'},\n",
       " {'movie': '14_Blades', 'director': 'Daniel Lee'},\n",
       " {'movie': '22_Bullets', 'director': 'Richard Berry'},\n",
       " {'movie': 'The_A-Team_(film)', 'director': 'Joe Carnahan'},\n",
       " {'movie': 'Alien_vs_Ninja', 'director': 'Seiji Chiba'},\n",
       " {'movie': 'Bad_Blood_(2010_film)', 'director': 'Dennis Law'},\n",
       " {'movie': 'Bangkok_Knockout', 'director': 'Panna Rittikrai'},\n",
       " {'movie': 'Blades_of_Blood', 'director': 'Lee Joon-ik'},\n",
       " {'movie': 'The_Book_of_Eli', 'director': 'Allen Hughes'},\n",
       " {'movie': 'The_Bounty_Hunter_(2010_film)', 'director': 'Andy Tennant'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = read_tsv()\n",
    "movies[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287/287 [01:47<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "statements=[]\n",
    "predicted_movies = {}\n",
    "for m in tqdm(movies):\n",
    "\n",
    "        txt = parse_wikipedia(m['movie'])\n",
    "        persons = find_PER_entities(txt)\n",
    "        director = find_director(txt, persons)\n",
    "        \n",
    "        if director != '':\n",
    "            statements.append(m['movie'] + ' is directed by ' + director + '.')\n",
    "            predicted_movies[m['movie']] = director\n",
    "        else: \n",
    "            predicted_movies[m['movie']] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {m['movie'] : m['director'] for m in movies}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Compute the precision and recall based on the given ground truth (column Director from tsv file) and show examples of statements that are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6550522648083623\n",
      "Recall: 0.6550522648083623\n",
      "\n",
      "***Sample Statements***\n",
      "['13_Assassins_(2010_film) is directed by Takashi Miike.', '14_Blades is directed by Daniel Lee.', '22_Bullets is directed by Richard Berry.', 'Alien_vs_Ninja is directed by Seiji Chiba.', 'Bad_Blood_(2010_film) is directed by Dennis Law.']\n"
     ]
    }
   ],
   "source": [
    "# compute precision and recall\n",
    "precision = sum([1 if predicted_movies[m] == gt[m] else 0 for m in predicted_movies.keys()]) / len(predicted_movies)\n",
    "recall = sum([1 if predicted_movies[m] == gt[m] else 0 for m in predicted_movies.keys()]) / len(gt)\n",
    "print ('Precision:',precision)\n",
    "print ('Recall:',recall)\n",
    "print('\\n***Sample Statements***')\n",
    "print(statements[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Named Entity Recognition using Hidden Markov Model\n",
    "\n",
    "\n",
    "Define a Hidden Markov Model (HMM) that recognizes Person (*PER*) entities.\n",
    "Particularly, your model must be able to recognize pairs of the form (*firstname lastname*) as *PER* entities.\n",
    "Using the given sentences as training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=['The best blues singer was Bobby Bland while Ray Charles pioneered soul music .', \\\n",
    "              'Bobby Bland was just a singer whereas Ray Charles was a pianist , songwriter and singer .' \\\n",
    "              'None of them lived in Chicago .']\n",
    "\n",
    "test_set=['Ray Charles was born in 1930 .', \\\n",
    "          'Bobby Bland was born the same year as Ray Charles .', \\\n",
    "          'Muddy Waters is the father of Chicago Blues .']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Annotate your training set with the labels I (for PER entities) and O (for non PER entities).\n",
    "\t\n",
    "    *Hint*: Represent the sentences as sequences of bigrams, and label each bigram.\n",
    "\tOnly bigrams that contain pairs of the form (*firstname lastname*) are considered as *PER* entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation\n",
      " ['O', 'O', 'O', 'O', 'O', 'I', 'O', 'O', 'I', 'O', 'O', 'O', 'O', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bigram Representation\n",
    "def getBigrams(sents):\n",
    "    return [b[0]+' '+b[1] for l in sents for b in zip(l.split(' ')[:-1], l.split(' ')[1:])]\n",
    "\n",
    "bigrams = getBigrams(training_set)\n",
    "\n",
    "#Annotation\n",
    "PER = ['Bobby Bland', 'Ray Charles']\n",
    "annotations = []\n",
    "for b in bigrams:\n",
    "    annotations.append('I' if b in PER else 'O')\n",
    "print('Annotation\\n', annotations,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Compute the transition and emission probabilities for the HMM (use smoothing parameter $\\lambda$=0.5).\n",
    "\n",
    "    *Hint*: For the emission probabilities you can utilize the morphology of the words that constitute a bigram (e.g., you can count their uppercase first characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probabilities\n",
      " {'P(I|start)': 0.11428571428571428, 'P(O|start)': 0.8857142857142857, 'P(O|O)': 0.7647058823529411, 'P(O|I)': 0.11764705882352941, 'P(I|O)': 0.11764705882352941, 'P(I|I)': 0.0} \n",
      "\n",
      "Emission Probabilities\n",
      " {'P(2_upper|O)': 0.125, 'P(2_upper|I)': 0.020833333333333332, 'P(1_upper|O)': 0.14583333333333334, 'P(1_upper|I)': 0.041666666666666664, 'P(0_upper|O)': 0.20833333333333334, 'P(0_upper|I)': 0.08333333333333333} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.5\n",
    "\n",
    "#Transition Probabilities\n",
    "transition_prob={}\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter(annotations)\n",
    "c2 = Counter(list(zip(annotations, annotations[1:])))\n",
    "\n",
    "#Prior\n",
    "transition_prob['P(I|start)'] = c[\"I\"] / c.total()\n",
    "transition_prob['P(O|start)'] = c['O'] / c.total()\n",
    "\n",
    "transition_prob['P(O|O)'] = (c2[('O','O')] / c2.total())  # this is wrong, only divide by the number of 'O'!!!\n",
    "transition_prob['P(O|I)'] = (c2[('I','O')] / c2.total())\n",
    "transition_prob['P(I|O)'] = (c2[('O','I')] / c2.total())\n",
    "transition_prob['P(I|I)'] = (c2[('I','I')] / c2.total())\n",
    "\n",
    "print('Transition Probabilities\\n',transition_prob, '\\n')\n",
    "\n",
    "#Emission Probabilities\n",
    "emission_prob={}\n",
    "\n",
    "uppers = []\n",
    "for b in bigrams:\n",
    "    c = 0\n",
    "    if b.split()[0][0].isupper():\n",
    "        c+=1\n",
    "    if b.split()[1][0].isupper():\n",
    "        c +=1\n",
    "    uppers.append(c)\n",
    "        \n",
    "default_emission = (1-lambda_) / len(bigrams)\n",
    "\n",
    "c = Counter(list(zip(uppers, annotations)))\n",
    "\n",
    "emission_prob['P(2_upper|O)'] = lambda_ * c[(2,'O')] / c.total() + default_emission  # same mistake as above !!!\n",
    "emission_prob['P(2_upper|I)'] = lambda_ * c[(2,'I')] / c.total() + default_emission\n",
    "emission_prob['P(1_upper|O)'] = lambda_ * c[(1,'O')] / c.total() + default_emission\n",
    "emission_prob['P(1_upper|I)'] = lambda_ * c[(1,'I')] / c.total() + default_emission\n",
    "emission_prob['P(0_upper|O)'] = lambda_ * c[(0,'O')] / c.total() + default_emission\n",
    "emission_prob['P(0_upper|I)'] = lambda_ * c[(0,'I')] / c.total() + default_emission\n",
    "\n",
    "print('Emission Probabilities\\n', emission_prob, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Predict the labels of the test set and compute the precision and the recall of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0023809523809523807 0.11071428571428571\n",
      "0.004901960784313725 0.11151960784313726\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.0024509803921568627 0.09558823529411764\n",
      "0.004901960784313725 0.11151960784313726\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.004901960784313725 0.11151960784313726\n",
      "0.0024509803921568627 0.09558823529411764\n",
      "0.004901960784313725 0.11151960784313726\n",
      "0.0024509803921568627 0.09558823529411764\n",
      "0.004901960784313725 0.11151960784313726\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.00980392156862745 0.15931372549019607\n",
      "0.004901960784313725 0.11151960784313726\n",
      "0.0024509803921568627 0.09558823529411764\n",
      "0.004901960784313725 0.11151960784313726\n",
      "Predicted Entities\n",
      " [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "bigrams = getBigrams(test_set)\n",
    "entities=[]\n",
    "prev_state='start'\n",
    "for b in bigrams:\n",
    "\n",
    "    c = 0\n",
    "    if b.split()[0][0].isupper():\n",
    "        c+=1\n",
    "    if b.split()[1][0].isupper():\n",
    "        c +=1\n",
    "\n",
    "    I_prob = transition_prob['P(I|{})'.format(prev_state)]*emission_prob['P({}_upper|I)'.format(c)]\n",
    "    O_prob = transition_prob['P(O|{})'.format(prev_state)]*emission_prob['P({}_upper|O)'.format(c)]\n",
    "\n",
    "    print(I_prob, O_prob)\n",
    "    \n",
    "    if O_prob > I_prob:\n",
    "        prev_state = 'O'\n",
    "    else:\n",
    "        entities.append(b)\n",
    "        prev_state = 'I'\n",
    "\n",
    "print('Predicted Entities\\n', entities, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is *...%* while recall is *...%*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Comment on how you can further improve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "228px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "67b2dac036e870edddccadcf0a7859a73125fc584e582e34d6a822a4260f8464"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
